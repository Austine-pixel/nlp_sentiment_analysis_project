






























































# Libraries for Data manipulation
import pandas as pd
import numpy as np

# Libaries for Visualization
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

# NLP tools Libraries
import re
import string
import nltk
from nltk.corpus import stopwords
from wordcloud import WordCloud
from collections import Counter
from nltk.tokenize import word_tokenize 

# Downloading NLTK resources 
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt')





# Loading the Sentiment140 dataset
cols = ['target', 'ids', 'date', 'flag', 'user', 'text']
df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1', names=cols)

# Preview first few rows
df.head()














# Keeping only sentiment label and tweet text
df = df[['target', 'text']]

# Renaming target: 0 -> negative, 4 -> positive
df = df[df['target'].isin([0, 4])]
df['sentiment'] = df['target'].map({0: 'negative', 4: 'positive'})

# Dropping the original target column
df.drop('target', axis=1, inplace=True)

# Checking class distribution
df['sentiment'].value_counts()





# Defining function to clean tweets
def clean_text(text):
    text = text.lower()  # Lowercase
    text = re.sub(r"http\S+|www\S+|https\S+", '', text)  # Remove URLs
    text = re.sub(r"@\w+", '', text)  # Remove mentions
    text = re.sub(r"#\w+", '', text)  # Remove hashtags
    text = re.sub(r"[^\w\s]", '', text)  # Remove punctuation
    text = re.sub(r"\d+", '', text)  # Remove numbers
    text = re.sub(r"\s{2,}", ' ', text)  # Remove multiple spaces
    return text.strip()

# Apply cleaning
df['clean_text'] = df['text'].apply(clean_text)

# Removing stopwords
stop_words = set(stopwords.words('english'))
df['clean_text'] = df['clean_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))

# Dropping empty clean texts
df = df[df['clean_text'].str.strip() != '']





# Adding tweet length feature(word count)
df['text_length'] = df['clean_text'].apply(lambda x: len(x.split()))

# Display stats
df.groupby('sentiment')['text_length'].describe()








# Assigning x variable to both x and hue
sns.countplot(data=df, x='sentiment', hue='sentiment', palette='viridis', legend=False)

plt.title("Distribution of Sentiment Classes")
plt.xlabel("Sentiment (0 = Negative, 4 = Positive)")
plt.ylabel("Tweet Count")
plt.grid(True, axis='y', linestyle='--', alpha=0.5)
plt.show()








plt.hist(df[df['sentiment'] == 'negative']['text_length'], bins=30, alpha=0.7, label='Negative', color='red')
plt.hist(df[df['sentiment'] == 'positive']['text_length'], bins=30, alpha=0.7, label='Positive', color='green')
plt.title("Tweet Length Distribution")
plt.xlabel("Number of Words")
plt.ylabel("Number of Tweets")
plt.legend()
plt.grid(True)
plt.show()








# Create a 'tokens' column by tokenizing the cleaned tweets
df['tokens'] = df['clean_text'].apply(word_tokenize)

# Getting top N words and their frequencies
def get_top_n_words(tokens_list, n=20):
    all_words = [word.lower() for tokens in tokens_list for word in tokens]
    return Counter(all_words).most_common(n)

# Function to plot top words for given sentiment
def plot_top_words(sentiment_label, color):
    tokens = df[df['sentiment'] == sentiment_label]['tokens']
    top_words = get_top_n_words(tokens)

    words, freqs = zip(*top_words)

    plt.figure(figsize=(10, 5))
    plt.barh(words[::-1], freqs[::-1], color=color)  # horizontal bar chart (top at top)
    plt.title(f"Top Words in {sentiment_label.capitalize()} Tweets")
    plt.xlabel("Frequency")
    plt.tight_layout()
    plt.show()

# Plot for both sentiments
plot_top_words('positive', color='green')


plot_top_words('negative', color='red')











from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')
X = vectorizer.fit_transform(df['clean_text'])  # Feature matrix
y = df['sentiment']                             # Target variable (0 or 4)





from sklearn.model_selection import train_test_split 

# Split into train and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)








# Importing libraries
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score





# Instantiate and train the Logistic Regression model
logreg = LogisticRegression(max_iter=1000)
logreg.fit(X_train, y_train)

# Predict on test set
y_pred_logreg = logreg.predict(X_test)





# Evaluate the model
print("Logistic Regression Results:")
print(confusion_matrix(y_test, y_pred_logreg))
print(classification_report(y_test, y_pred_logreg))
print("Accuracy:", accuracy_score(y_test, y_pred_logreg))











from sklearn.naive_bayes import MultinomialNB





# Instantiate and train the Naive Bayes model
nb = MultinomialNB()
nb.fit(X_train, y_train)

# Predict on test set
y_pred_nb = nb.predict(X_test)





print("Naive Bayes Results:")
print(confusion_matrix(y_test, y_pred_nb))
print(classification_report(y_test, y_pred_nb))
print("Accuracy:", accuracy_score(y_test, y_pred_nb))











from sklearn.ensemble import RandomForestClassifier





#  Sampling 500,000 rows from the main DataFrame for faster model training
df_sample = df.sample(n=500000, random_state=42)

# Defining features and labels from the sampled dataset
X_sample_text = df_sample['clean_text']   # Feature text
y_sample = df_sample['sentiment']         # Labels (target variable)

# TF-IDF Vectorization on the sampled tweets
vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
X = vectorizer.fit_transform(X_sample_text)  # Convert text to numerical feature vectors
y = y_sample                                 # Set target labels





# Split the dataset (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)





# Instantiate the Random Forest Classifier
rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
rf.fit(X_train, y_train)

# Predict sentiment labels on the test set
y_pred_rf = rf.predict(X_test)





# Evaluate the model
print("Random Forest Results (500K Sample):")
print(confusion_matrix(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))
print("Accuracy:", accuracy_score(y_test, y_pred_rf))








# Getting feature names from TF-IDF
feature_names = vectorizer.get_feature_names_out()

# Get importances and sort
importances = rf.feature_importances_
indices = np.argsort(importances)[-20:]  # Top 20 important features

# Plot
plt.figure(figsize=(10, 8))
plt.barh(range(len(indices)), importances[indices], align="center", color='skyblue')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.title("Top 20 Important Words (Random Forest)")
plt.xlabel("Feature Importance Score")
plt.tight_layout()
plt.show()








from sklearn.model_selection import GridSearchCV

# Defining the model
logreg = LogisticRegression(solver='liblinear')  

# Defining the hyperparameter grid
param_grid = {
    'C': [0.01, 0.1, 1, 10],            # Regularization strength
    'penalty': ['l1', 'l2']             # Type of regularization
}

# Setting up GridSearchCV
grid_search = GridSearchCV(
    estimator=logreg,
    param_grid=param_grid,
    cv=3,                               # 3-fold cross-validation
    scoring='accuracy',
    verbose=1,
    n_jobs=-1
)

# Fit the model
grid_search.fit(X_train, y_train)

# Show best parameters and accuracy
print("Best Parameters:", grid_search.best_params_)
print("Best Cross-Validated Accuracy:", grid_search.best_score_)





# Evaluate the best model on test set
best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(X_test)

print("Classification Report (Best Logistic Regression):")
print(classification_report(y_test, y_pred_best))
print("Accuracy:", accuracy_score(y_test, y_pred_best))











# sampling 500k rows
df_sample = df.sample(n=500000, random_state=42)

X_sample_text = df_sample['clean_text']
y_sample = df_sample['sentiment']

# TF-IDF Vectorization
vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
X_sample = vectorizer.fit_transform(X_sample_text)

# Final train-test split for ALL models
X_train_sample, X_test_sample, y_train_sample, y_test_sample = train_test_split(
    X_sample, y_sample, test_size=0.2, random_state=42, stratify=y_sample
)





# 1. Logistic Regresstion
logreg = LogisticRegression(max_iter=1000)
logreg.fit(X_train_sample, y_train_sample)
y_pred_logreg = logreg.predict(X_test_sample)


# 2. Naive Bayes
nb = MultinomialNB()
nb.fit(X_train_sample, y_train_sample)
y_pred_nb = nb.predict(X_test_sample)


# 3. Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train_sample, y_train_sample)
y_pred_rf = rf.predict(X_test_sample)


# Ensure all predictions are done on X_test_sample and y_test_sample
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

models = {
    "Logistic Regression": y_pred_logreg,
    "Naive Bayes": y_pred_nb,
    "Random Forest": y_pred_rf
}

metrics = {
    "Model": [],
    "Accuracy": [],
    "Precision": [],
    "Recall": [],
    "F1-Score": []
}

# Looping through each model's predictions
for model_name, y_pred in models.items():
    metrics["Model"].append(model_name)
    metrics["Accuracy"].append(accuracy_score(y_test_sample, y_pred))
    metrics["Precision"].append(precision_score(y_test_sample, y_pred, pos_label='positive'))
    metrics["Recall"].append(recall_score(y_test_sample, y_pred, pos_label='positive'))
    metrics["F1-Score"].append(f1_score(y_test_sample, y_pred, pos_label='positive'))

# Converting to DataFrame for display
results_df = pd.DataFrame(metrics)
results_df.sort_values(by="Accuracy", ascending=False, inplace=True)
print(results_df)





# Plot setup
fig, ax = plt.subplots(figsize=(12, 6))
bar_width = 0.2
index = range(len(results_df))

# Plotting bars for each metric
plt.bar([i - bar_width*1.5 for i in index], results_df['Accuracy'], bar_width, label='Accuracy')
plt.bar([i - bar_width/2 for i in index], results_df['Precision'], bar_width, label='Precision')
plt.bar([i + bar_width/2 for i in index], results_df['Recall'], bar_width, label='Recall')
plt.bar([i + bar_width*1.5 for i in index], results_df['F1-Score'], bar_width, label='F1-Score')

# Labeling
plt.xlabel('Model')
plt.ylabel('Score')
plt.title('Model Performance Comparison')
plt.xticks(index, results_df['Model'])
plt.ylim(0.7, 0.8)  # Optional: adjust based on scores
plt.legend()
plt.grid(axis='y')
plt.tight_layout()
plt.show()








from sklearn.metrics import confusion_matrix

# Creating a 1x3 subplot for 3 models
fig, axes = plt.subplots(1, 3, figsize=(16, 5))

# Defining titles and predictions
model_titles = ['Logistic Regression', 'Naive Bayes', 'Random Forest']
model_preds = [y_pred_logreg, y_pred_nb, y_pred_rf]
true_labels = y_test_sample

# Loop to create each confusion matrix plot
for i, (title, preds) in enumerate(zip(model_titles, model_preds)):
    cm = confusion_matrix(true_labels, preds)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'],
                yticklabels=['Negative', 'Positive'], ax=axes[i])
    axes[i].set_title(f'{title}')
    axes[i].set_xlabel('Predicted')
    axes[i].set_ylabel('Actual')

plt.tight_layout()
plt.show()











# Split raw text and labels 
X_sample_text_train, X_sample_text_test, y_train_sample, y_test_sample = train_test_split(
    df_sample['clean_text'], df_sample['sentiment'], test_size=0.2, random_state=42, stratify=df_sample['sentiment']
)

# Refit vectorizer on training set and transform both
vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)

X_train = vectorizer.fit_transform(X_sample_text_train)
X_test = vectorizer.transform(X_sample_text_test)

X_test_sample_text = X_sample_text_test.reset_index(drop=True) 


error_df = pd.DataFrame({
    'text': X_test_sample_text,  
    'true_label': y_test_sample.reset_index(drop=True),
    'logreg_pred': y_pred_logreg,
    'nb_pred': y_pred_nb,
    'rf_pred': y_pred_rf
})

# Filter misclassified tweets by Logistic Regression
logreg_errors = error_df[error_df['true_label'] != error_df['logreg_pred']]
logreg_errors.head(10)





















